# -*- coding: utf-8 -*-
"""caching.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R_B05AlcGlznBNUO5eeYTiF2lYBS-ZGv
"""

# Step 1️⃣: Import and Create SparkSession
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Caching-Demo") \
    .master("local[*]") \
    .getOrCreate()

# Step 2️⃣: Sample DataFrame create karo (Fake Employee Data)
data = [
    (1, "Alice", "HR", 5000),
    (2, "Bob", "IT", 7000),
    (3, "Charlie", "HR", 5500),
    (4, "David", "Finance", 6000),
    (5, "Eva", "IT", 7200)
]

columns = ["id", "name", "department", "salary"]

df = spark.createDataFrame(data, columns)
df.show()

# Step 3️⃣: Transformation (expensive operation) — Grouping
grouped_df = df.groupBy("department").sum("salary")

# Step 5️⃣: Trigger Action (First Time — Slow because it computes)
grouped_df.show()  # caching starts here

# Step 7️⃣: Unpersist (optional) — agar memory free karni ho
grouped_df.unpersist()

from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Caching-With-Large-Dataset") \
    .master("local[*]") \
    .getOrCreate()

df = spark.read.csv("hw_200.csv", header=True, inferSchema=True)
df.printSchema()
df.show(5)

df_transformed = df.withColumn("bmi", df[" \"Weight(Pounds)\""] / (df[" Height(Inches)\""] ** 2))
df_transformed.show(5)

df_transformed.cache()

import time

start1 = time.time()
df_transformed.select("bmi").describe().show()
print("⏱️ First Run Duration:", time.time() - start1, "seconds")

start2 = time.time()
df_transformed.select("bmi").describe().show()
print("⚡ Second Run Duration:", time.time() - start2, "seconds")

df_transformed.unpersist()

