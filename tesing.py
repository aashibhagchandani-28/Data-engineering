# -*- coding: utf-8 -*-
"""Tesing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t_11ymfuwTxyONLtPZNFjXCKP8h-sv0H
"""

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("BeginnerTheory").getOrCreate()

data = [(1, 100), (2, 200)]
df = spark.createDataFrame(data, ["id", "amount"])
df_transformed = df.withColumn("amount_doubled", df["amount"] * 2)
df_transformed.show()

input_data = [(1, 10), (2, 20)]
output_data = [(1, 100), (2, 200)]

df_input = spark.createDataFrame(input_data, ["id", "amount"])
df_output = spark.createDataFrame(output_data, ["id", "amount"])

# Reconcile
reconciled = df_input.join(df_output, on="id").withColumn("matched", df_input["amount"] * 10 == df_output["amount"])
reconciled.show()

from pyspark.sql.functions import col

df = spark.createDataFrame([(1, 50), (2, None), (3, -10)], ["id", "amount"])

# Stage 1
df_stage1 = df.withColumn("amount", col("amount") * 10)
df_stage1_validated = df_stage1.filter(col("amount").isNotNull() & (col("amount") >= 0))
df_stage1_validated.show()

from pyspark.sql import SparkSession
import pytest

@pytest.fixture(scope="session")
def spark():
    return SparkSession.builder.master("local[*]").appName("TestPipeline").getOrCreate()

def test_transformation(spark):
    data = [(1, 10), (2, 20)]
    df = spark.createDataFrame(data, ["id", "amount"])
    df_transformed = df.withColumn("amount", df["amount"] * 10)
    result = [row["amount"] for row in df_transformed.collect()]
    assert result == [100, 200]

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, md5, concat_ws

# Spark session initialize karna
spark = SparkSession.builder \
    .appName("Data Reconciliation Techniques") \
    .getOrCreate()

input_data = [
    (1, "Alice", 100.0),
    (2, "Bob", 200.0),
    (3, "Charlie", 300.0)
]

# Output data: transformed or processed data
output_data = [
    (1, "Alice", 100.0),
    (2, "Bob", 200.0),
    (3, "Charlie", 300.0)
]

columns = ["id", "name", "amount"]

# DataFrames create karna using input data
df_input = spark.createDataFrame(input_data, columns)
df_output = spark.createDataFrame(output_data, columns)

input_count = df_input.count()
output_count = df_output.count()

print(f"Row Count Check: Input={input_count}, Output={output_count}")
if input_count == output_count:
    print("PASS: Row count matches")
else:
    print("FAIL: Row count mismatch")

df_input_hashed = df_input.withColumn("row_hash", md5(concat_ws("||", *df_input.columns)))
df_output_hashed = df_output.withColumn("row_hash", md5(concat_ws("||", *df_output.columns)))

# collect() se hash list nikaalna
input_hashes = df_input_hashed.select("row_hash").collect()
output_hashes = df_output_hashed.select("row_hash").collect()

if input_hashes == output_hashes:
    print("PASS: Hash comparison passed (data is same)")
else:
    print("FAIL: Hash mismatch (data modified)")

duplicate_input = df_input.groupBy("id").count().filter(col("count") > 1).count()
duplicate_output = df_output.groupBy("id").count().filter(col("count") > 1).count()

print(f"Duplicate IDs in Input: {duplicate_input}")
print(f"Duplicate IDs in Output: {duplicate_output}")

if duplicate_input == 0 and duplicate_output == 0:
    print("PASS: No duplicate primary keys")
else:
    print("FAIL: Duplicate primary keys found")

invalid_input = df_input.filter((col("amount") <= 0) | (col("amount").isNull())).count()
invalid_output = df_output.filter((col("amount") <= 0) | (col("amount").isNull())).count()

if invalid_input == 0 and invalid_output == 0:
    print("PASS: Column-level validation passed")
else:
    print("FAIL: Invalid values in 'amount' column")

invalid_name_input = df_input.filter((col("name").isNull()) | (col("name") == "")).count()
invalid_name_output = df_output.filter((col("name").isNull()) | (col("name") == "")).count()

if invalid_name_input == 0 and invalid_name_output == 0:
    print("PASS: Business rule check passed")
else:
    print("FAIL: Business rule violation - Name is missing or invalid")

