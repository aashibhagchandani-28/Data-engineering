# -*- coding: utf-8 -*-
"""azure.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L-ginnxvQwrxQwGE9peRsq4vOOrvGc-u
"""

import pandas as pd

df = pd.read_csv("dummy_sales.csv")

df_filtered = df[df["amount"] > 5000]

df_filtered.rename(columns={"amount": "sale_amount"}, inplace=True)

df_filtered.to_csv("output_filtered_sales.csv", index=False)

print("✅ Mock Pipeline Completed")

# Step 1: Import SparkSession & Functions
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

spark = SparkSession.builder.appName("RealPipeline").getOrCreate()

df = spark.read.csv("sales_data.csv", header=True, inferSchema=True)

df_filtered = df.filter(col("amount") > 5000)

df_renamed = df_filtered.withColumnRenamed("amount", "sale_amount")

df_renamed.write.csv("output/final_data.csv", header=True, mode="overwrite")

print("✅ PySpark Pipeline Completed")

import pandas as pd

df = pd.read_csv("dummy_sales.csv")
print("🔍 Original Data:")
print(df)

df_filtered = df[df["amount"] > 5000]

df_filtered.rename(columns={"amount": "sale_amount"}, inplace=True)

print("\n✅ Filtered + Renamed Data:")
print(df_filtered)

# --------------------------------------------
spark = SparkSession.builder \
    .appName("SalesPipeline") \
    .getOrCreate()

f = spark.read.csv("sales_data.csv", header=True, inferSchema=True)

print("🔍 Original Data:")

df_filtered = f.filter(col("amount") > 5000)

df_renamed = df_filtered.withColumnRenamed("amount", "sale_amount")

print("✅ Filtered + Renamed Data:")
df_renamed.show()

df_renamed.write.csv("output/final_data.csv", header=True, mode="overwrite")

