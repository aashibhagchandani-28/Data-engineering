# -*- coding: utf-8 -*-
"""parallesim.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IRVzPluxHPDG6oPCtFc333sxhiYSAzZ0
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col 

# Spark session
spark = SparkSession.builder \
    .appName("Parallelism-Practical") \
    .config("spark.sql.shuffle.partitions", "4") \
    .getOrCreate()

# Sample data
data = [(i, f"User_{i}") for i in range(1, 100001)]  # 1 lakh rows
columns = ["id", "name"]

df = spark.createDataFrame(data, columns)

# Show basic info
df.show(5)

# Check number of partitions
print("Initial partitions:", df.rdd.getNumPartitions())

# Repartition to 8 partitions
df_repart = df.repartition(8)

print("After repartition:", df_repart.rdd.getNumPartitions())

# Action to trigger job (see in Spark UI)
df_repart.groupBy("name").count().show()

# Coalesce to 2 partitions (no shuffle)
df_coal = df.coalesce(2)

print("After coalesce:", df_coal.rdd.getNumPartitions())

# Trigger action
df_coal.groupBy("name").count().show()

spark.conf.set("spark.sql.shuffle.partitions", "10")

# Trigger shuffle operation
df.groupBy("id").count().show()

df2 = df.repartition(4)  # 4 new partitions

df2

df3 = df.coalesce(1)  # All partitions into 1

#use cases of each



