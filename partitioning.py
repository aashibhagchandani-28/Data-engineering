# -*- coding: utf-8 -*-
"""partitioning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KFJaFu7uPFQRR-__yDOdJukBTr8GKDei
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Spark session
spark = SparkSession.builder \
    .appName("Partitioning-Practical") \
    .getOrCreate()

# Sample data
data = [
   (1, "Aarav", "India", 2021),
(2, "Liam", "USA", 2023),
(3, "Noah", "UK", 2022),
(4, "Oliver", "India", 2024),
(5, "Elijah", "Canada", 2021),
(6, "William", "Australia", 2020),
(7, "James", "USA", 2020),
(8, "Benjamin", "Germany", 2023),
(9, "Lucas", "India", 2022),
(10, "Henry", "UK", 2021),
(11, "Alexander", "Canada", 2024),
(12, "Mason", "Australia", 2023),
(13, "Michael", "Germany", 2020),
(14, "Ethan", "USA", 2022),
(15, "Daniel", "India", 2021),
(16, "Jacob", "UK", 2020),
(17, "Logan", "Canada", 2023),
(18, "Jackson", "Australia", 2022),
(19, "Levi", "Germany", 2021),
(20, "Sebastian", "India", 2023),
(21, "Mateo", "USA", 2024),
(22, "Jack", "UK", 2022),
(23, "Owen", "Germany", 2020),
(24, "Theodore", "Canada", 2021),
(25, "Aiden", "India", 2020),
(26, "Samuel", "USA", 2023),
(27, "Joseph", "Australia", 2021),
(28, "John", "UK", 2024),
(29, "David", "Canada", 2020),
(30, "Wyatt", "Germany", 2022),
(31, "Matthew", "India", 2024),
(32, "Luke", "USA", 2021),
(33, "Asher", "UK", 2023),
(34, "Carter", "Australia", 2022),
(35, "Julian", "Germany", 2024),
(36, "Grayson", "Canada", 2023),
(37, "Leo", "India", 2022),
(38, "Jayden", "USA", 2020),
(39, "Gabriel", "Australia", 2024),
(40, "Isaac", "UK", 2021),
(41, "Lincoln", "India", 2020),
(42, "Anthony", "Germany", 2021),
(43, "Hudson", "Canada", 2022),
(44, "Dylan", "USA", 2023),
(45, "Ezra", "UK", 2024),
(46, "Thomas", "Australia", 2021),
(47, "Charles", "Germany", 2020),
(48, "Christopher", "India", 2023),
(49, "Jaxon", "Canada", 2024),
(50, "Maverick", "USA", 2022),
]

columns = ["id", "name", "country","year"]

df = spark.createDataFrame(data, columns)

df.show(50, truncate=False)

# Partition by 'country' column
df.write.mode("overwrite").partitionBy("country").parquet("/tmp/partitioned_data")

# Read the partitioned data
df2 = spark.read.parquet("/tmp/partitioned_data")

# Filter only India (partition pruning will happen)
df2.filter(col("country") == "India").show(50, truncate=False)

df3 = df.repartition(5, "country").show(50, truncate=False)

df4 = df.coalesce(2)
df4.show(50, truncate=False)

