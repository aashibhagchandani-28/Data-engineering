# -*- coding: utf-8 -*-
"""errors.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13Hx_1dhBLrQruAVgrh2z4uJjypQJxEQV
"""

print("Hello"

x = 5 / 0

file = open("notfound.txt")

def add(a, b):
    return a - b   # Mistakenly wrote minus

print(add(5, 2))   # Output: 3, but expected: 7

# Defined schema expecting 'id' to be Integer
schema = StructType([StructField("id", IntegerType(), True)])
df = spark.read.csv("data.csv", schema=schema)

# But if 'data.csv' has id = "abc", Spark throws an error

df1.join(df2, "id").groupBy("category").count()

df = spark.read.csv("/mnt/data/my_file.csv")

df.select("address").filter(col("address").isNull()).show()

from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()

try:
    df = spark.read.csv("wrong_path/file.csv", header=True)
    df.show()
except Exception as e:
    print("‚ö†Ô∏è File not found or invalid path.")
    print("Error details:", e)

from pyspark.sql.functions import col

df = spark.createDataFrame([
    (1, 5000, 500),
    (2, None, 400),
], ["id", "salary", "bonus"])

try:
    df = df.withColumn("percent_bonus", col("bonus") / col("salary"))
    df.show()
except Exception as e:
    print("‚ö†Ô∏è Division by null or zero detected.")
    print("Error:", e)

try:
    # Some risky operation
    pass
except FileNotFoundError as fe:
    print("File not found:", fe)
except ZeroDivisionError as ze:
    print("Division by zero:", ze)
except Exception as e:
    print("Some other error occurred:", e)

import logging
logging.basicConfig(level=logging.ERROR)

try:
    df = spark.read.csv("wrong_path.csv", header=True)
except Exception as e:
    logging.error("üî• Spark Error: %s", e)

class EmptyDataError(Exception):
    pass

def check_data(df):
    if df.count() == 0:
        raise EmptyDataError("Bhai! Data hi nahi hai!")

try:
    check_data(df)
except EmptyDataError as e:
    print("Apna banaya hua error:", e)

class SchemaMismatchError(Exception):
    pass

def validate_schema(df, expected):
    if set(df.columns) != set(expected):
        raise SchemaMismatchError("Column set galat hai!")

try:
    validate_schema(df, ["id", "name", "email"])
except SchemaMismatchError as e:
    print("Schema issue:", e)

print("‚úÖ Step 1: Data Load Check")
df.show(5)  # View first 5 rows

print("‚úÖ Step 2: Schema Check")
df.printSchema()  # View column structure

print("‚úÖ Step 3: Null Check")
from pyspark.sql.functions import col, count, when
df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()

try:
    df2 = df.withColumn("new_col", df["amount"] / df["count"])
    df2.show()
except Exception as e:
    print("‚ö†Ô∏è Error in transformation:", e)

df = spark.read.csv("/mnt/data/non_existing_file.csv", header=True, inferSchema=True)
df.show()

df = spark.createDataFrame([(1, None), (2, "John")], ["id", "name"])
df = df.withColumn("name_length", df["name"].substr(1, 3))
df.show()

df = spark.createDataFrame([(1, "Apple")], ["id", "product"])
df.select("price").show()

class MyCustomClass:
    def __init__(self, value):
        self.value = value

    def process(self, x):
        return x + self.value

obj = MyCustomClass(5)

rdd = spark.sparkContext.parallelize([1, 2, 3])
result = rdd.map(lambda x: obj.process(x)).collect()
print(result)

